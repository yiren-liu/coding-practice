# Clustering Algorithms

This directory contains implementations of classical machine learning clustering algorithms.

## Available Problems

### K-Means Clustering
**File**: `kmeans.py`

A comprehensive implementation covering three K-Means variants and evaluation metrics:

#### 1. Standard K-Means
- **Algorithm**: Lloyd's algorithm (Expectation-Maximization)
- **Complexity**: O(nÂ·kÂ·dÂ·i) where n=samples, k=clusters, d=features, i=iterations
- **Best for**: Small to medium datasets (< 10K samples)
- **Pros**: Simple, interpretable, guaranteed convergence
- **Cons**: Sensitive to initialization, can be slow on large data

#### 2. K-Means++ Initialization
- **Key Innovation**: Smart centroid initialization
- **How it works**: 
  1. Choose first centroid randomly
  2. For remaining centroids, choose with probability âˆ D(x)Â²
  3. D(x) = distance to nearest existing centroid
- **Benefits**: 
  - Faster convergence (fewer iterations)
  - Better clustering quality
  - More consistent results
  - O(log k) approximation guarantee
- **Used in**: scikit-learn default, Google's MapReduce

#### 3. Mini-Batch K-Means
- **Key Innovation**: Stochastic gradient descent with mini-batches
- **How it works**: Update centroids using small random batches
- **Benefits**:
  - 3-10x faster than standard K-Means
  - Lower memory usage
  - Online learning capability
  - Scales to millions of samples
- **Trade-off**: ~5% clustering quality loss (usually acceptable)
- **Used in**: Large-scale image processing, real-time systems

## Performance Comparison

### Convergence Speed
```
Dataset: 300 samples, 3 clusters

Standard K-Means:  5 iterations, inertia: 1420.24
K-Means++:         2 iterations, inertia:  138.58  â† Better!
```

**Key Insight**: K-Means++ consistently finds better local minima and converges faster.

### Scalability
```
Dataset: 3000 samples, 3 clusters

Standard K-Means:      time: ~10ms
Mini-Batch K-Means:    time: ~3ms   â† 3x faster!
```

**Key Insight**: Speed advantage increases with dataset size. At 100K+ samples, Mini-Batch can be 10x faster.

## Evaluation Metrics

### 1. Inertia (Within-Cluster Sum of Squares)
```python
inertia = Î£ ||x - centroid[cluster(x)]||Â²
```
- **Lower is better** (tighter clusters)
- **Use for**: Model selection, convergence checking
- **Limitation**: Always decreases as k increases

### 2. Elbow Method
```python
results = elbow_method(X, range(2, 11))
# Plot k vs inertia, look for "elbow"
```
- **Purpose**: Find optimal number of clusters
- **How**: Plot inertia vs k, look for sharp bend
- **Example**: Inertia drops quickly until k=3, then slowly â†’ k=3 is optimal

### 3. Silhouette Score
```python
score = silhouette_score(X, labels)  # Range: [-1, 1]
```
- **Interpretation**:
  - **+1**: Point well-matched to cluster, far from others
  - **0**: Point on border between clusters
  - **-1**: Point may be in wrong cluster
- **Average > 0.5**: Good clustering
- **Average > 0.7**: Excellent clustering

## Real-World Applications

### 1. Customer Segmentation
```python
# Example: E-commerce
features = [purchase_frequency, avg_order_value, recency]
kmeans = KMeansPlusPlus(n_clusters=5)
customer_segments = kmeans.fit_predict(customer_data)

# Segments might be: VIP, Regular, Occasional, At-Risk, New
```

**Business Value**: 
- Targeted marketing campaigns
- Personalized recommendations
- Churn prediction

### 2. Image Compression
```python
# Reduce image colors from 16M to 16 colors
img_pixels = img.reshape(-1, 3)  # RGB values
kmeans = MiniBatchKMeans(n_clusters=16)
compressed = kmeans.fit_predict(img_pixels)

# Replace each pixel with its cluster centroid
compressed_img = kmeans.centroids[compressed].reshape(img.shape)
```

**Result**: 
- ~50% file size reduction
- Minimal visual quality loss
- Used in GIF, PNG compression

### 3. Document Clustering
```python
# Group similar documents
doc_embeddings = model.encode(documents)  # e.g., BERT embeddings
kmeans = KMeansPlusPlus(n_clusters=10)
topics = kmeans.fit_predict(doc_embeddings)
```

**Use Cases**:
- News article categorization
- Email organization
- Search result grouping

### 4. Anomaly Detection
```python
kmeans = KMeans(n_clusters=k)
kmeans.fit(normal_data)

# Points far from all centroids are anomalies
distances = torch.cdist(new_data, kmeans.centroids).min(dim=1)[0]
anomalies = distances > threshold
```

**Applications**:
- Fraud detection
- Network intrusion detection
- Manufacturing defect detection

### 5. Feature Engineering
```python
# Use cluster assignments as features
kmeans = KMeans(n_clusters=100)
cluster_features = kmeans.fit_predict(X)

# Or use distances to centroids
distance_features = torch.cdist(X, kmeans.centroids)

# Feed to downstream ML model
model.fit(distance_features, y)
```

**Benefits**: Captures non-linear relationships, works well with linear models

## Implementation Guide

### Standard K-Means Implementation

```python
class KMeans:
    def fit(self, X):
        # 1. Initialize centroids randomly
        indices = torch.randperm(n_samples)[:k]
        centroids = X[indices]
        
        # 2. Iterate until convergence
        for iter in range(max_iters):
            # E-step: Assign to nearest centroid
            distances = torch.cdist(X, centroids)
            labels = torch.argmin(distances, dim=1)
            
            # M-step: Update centroids
            for k in range(n_clusters):
                centroids[k] = X[labels == k].mean(dim=0)
            
            # Check convergence
            if centroid_shift < tolerance:
                break
```

### K-Means++ Implementation

```python
# Initialize first centroid randomly
centroids = [X[random_index]]

# Choose remaining centroids
for _ in range(k - 1):
    # Compute DÂ²(x) for each point
    distances = torch.cdist(X, torch.stack(centroids))
    min_distances = distances.min(dim=1)[0]
    D_squared = min_distances ** 2
    
    # Sample with probability âˆ DÂ²
    probabilities = D_squared / D_squared.sum()
    next_idx = torch.multinomial(probabilities, 1)
    centroids.append(X[next_idx])
```

### Mini-Batch K-Means Update

```python
# Initialize per-centroid counts
counts = torch.zeros(k)

for iteration in range(max_iters):
    # Sample mini-batch
    batch = X[random_indices]
    
    # Assign batch to centroids
    labels = assign_to_nearest(batch, centroids)
    
    # Update centroids with moving average
    for cluster_id in range(k):
        points = batch[labels == cluster_id]
        if len(points) > 0:
            counts[cluster_id] += len(points)
            lr = 1.0 / counts[cluster_id]
            
            # Moving average update
            centroids[cluster_id] = (
                (1 - lr) * centroids[cluster_id] + 
                lr * points.mean(dim=0)
            )
```

## Common Pitfalls & Solutions

### Problem 1: Poor Initialization
**Symptom**: Different runs give very different results
**Solution**: Always use K-Means++ initialization

### Problem 2: Choosing k
**Symptom**: Don't know how many clusters to use
**Solutions**:
1. **Elbow method**: Plot inertia vs k, look for bend
2. **Silhouette analysis**: Try multiple k, pick highest silhouette score
3. **Domain knowledge**: Use business requirements
4. **Gap statistic**: Compare inertia to random data

### Problem 3: Empty Clusters
**Symptom**: Some clusters have no points assigned
**Solutions**:
1. Keep old centroid (don't update)
2. Reinitialize from point furthest from any centroid
3. Use K-Means++ which reduces this issue

### Problem 4: Non-spherical Clusters
**Symptom**: K-Means performs poorly on elongated/irregular clusters
**Solutions**:
1. Consider **DBSCAN** or **Spectral Clustering** instead
2. Preprocess: Apply PCA or feature engineering
3. Use kernel K-Means for non-linear patterns

### Problem 5: Large Datasets
**Symptom**: K-Means too slow
**Solutions**:
1. Use **Mini-Batch K-Means** (3-10x speedup)
2. Downsample data for initialization, then assign all points
3. Use approximate nearest neighbors (FAISS library)

## Choosing the Right Variant

### Decision Tree

```
Dataset Size?
â”œâ”€ < 10K samples
â”‚  â””â”€ Use: K-Means++ (best quality)
â”‚
â”œâ”€ 10K - 100K samples  
â”‚  â””â”€ Use: K-Means++ or Mini-Batch (depending on speed needs)
â”‚
â””â”€ > 100K samples
   â””â”€ Use: Mini-Batch K-Means (much faster)

Quality Priority?
â”œâ”€ Critical â†’ K-Means++
â””â”€ Speed matters â†’ Mini-Batch

Online Learning?
â””â”€ Yes â†’ Mini-Batch K-Means (supports streaming data)
```

## Testing Your Implementation

```bash
# Navigate to project root
cd /path/to/coding-practice

# Activate virtual environment  
source .venv/bin/activate

# Test problem (should show TODOs not implemented)
python ml/problems/clustering/kmeans.py

# Test solution (should pass all tests)
python ml/solutions/clustering/kmeans.py

# Visualize results (after implementation)
python ml/problems/clustering/kmeans_visualization.py
```

## Expected Output

When your implementation is correct, you should see:

```
======================================================================
Part 1: Standard K-Means Tests
======================================================================
âœ“ K-Means fits successfully
âœ“ K-Means creates correct number of clusters
âœ“ K-Means assigns labels to all points
âœ“ K-Means converges
âœ“ K-Means computes valid inertia
âœ“ K-Means predicts new points

Tests passed: 6/6

... (similar for other parts)

Performance Comparison:
  K-Means++:  Better inertia, fewer iterations
  Mini-Batch: 3-10x faster, slight quality loss
```

## Extensions & Advanced Topics

### 1. K-Means Variants
- **Fuzzy C-Means**: Soft cluster assignments
- **K-Medoids (PAM)**: Use actual points as centers (more robust to outliers)
- **Kernel K-Means**: Non-linear clustering
- **Spherical K-Means**: For normalized/directional data

### 2. Hierarchical Clustering
- Agglomerative (bottom-up)
- Divisive (top-down)
- Dendrograms for visualization

### 3. Density-Based Clustering
- **DBSCAN**: Finds arbitrary-shaped clusters
- **HDBSCAN**: Hierarchical DBSCAN
- **OPTICS**: Ordering points for cluster structure

### 4. Probabilistic Clustering
- **Gaussian Mixture Models (GMM)**: Soft clustering with probabilities
- **EM algorithm**: General framework for clustering

## References

### Papers
- **K-Means**: Lloyd (1957) - "Least squares quantization in PCM"
- **K-Means++**: Arthur & Vassilvitskii (2007) - [Stanford paper](https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf)
- **Mini-Batch**: Sculley (2010) - [Google paper](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)

### Libraries
- **scikit-learn**: Industry-standard implementation
- **FAISS**: Facebook's fast clustering for billion-scale data
- **cuML**: GPU-accelerated K-Means (RAPIDS)

### Books
- "Pattern Recognition and Machine Learning" (Bishop) - Chapter 9
- "The Elements of Statistical Learning" (Hastie) - Chapter 14
- "Clustering Algorithms" (Xu & Wunsch) - Comprehensive survey

## Quick Reference

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| Inertia | Î£â€–x - Î¼â€–Â² | Lower = tighter clusters |
| Silhouette | (b-a)/max(a,b) | [-1,1], higher = better |
| Davies-Bouldin | Avg ratio of within to between | Lower = better |
| Calinski-Harabasz | Between / Within variance | Higher = better |

| Parameter | Typical Range | Notes |
|-----------|---------------|-------|
| n_clusters | 2-20 | Use elbow/silhouette to choose |
| max_iters | 100-300 | Usually converges < 50 iters |
| tol | 1e-4 to 1e-6 | Convergence threshold |
| batch_size | 100-1000 | For Mini-Batch, larger = more stable |

---

**Ready to implement?** Start with `kmeans.py` and follow the TODOs! ðŸš€

